<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Quin McGaugh and Otis Milliken AI WEBSITE</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Middlebury Relationship <br/> Predictor</h1>
        <p>AI Final Project by Otis Milliken and Quin McGaugh</p>
        <p>Using Decision Trees with Random Forrest to Predict Relationship Status</p>
        <p class="view"><em>View the Final Paper on <a href="https://docs.google.com/document/d/1O1phRgtGrBLben_A7D-Lv9dGYZ7l5m8MHdMpZKAGLLM/edit?usp=sharing">Google Docs. </a></em></p>
        <p class="view"><em>View the Project on <a href="https://github.com/quinmcg/FinalProjectAI">GitHub. </a></em></p>
        <p class="view"><em>View the <a href="https://docs.google.com/document/d/1tBR2TCfxEjODB6rnanns1hJPE4ViXXdfLoWQg86Gmlk/edit">Project Proposal</a>.</em></p>
        <p class="view"><em>View the <a href="https://forms.gle/n6rmRUudiC1quVNW7">Survey Form</a>.</em></p>
      </header>
      <section>
        <h1>Try Out Our Project</h1>
        <p> We'd love for you to try out our project! To do so, please follow the steps below. Feel free to email us if you run into any issues </p>
        <ol>
          <li>
            <p> To try out the project clone the <a href="https://docs.google.com/document/d/1tBR2TCfxEjODB6rnanns1hJPE4ViXXdfLoWQg86Gmlk/edit">repository</a> </p>
          </li>
          <li>
            <p>Some libraries are required to run this project. Please run the following commands in your terminal</p>
            <pre><code>pip install sklearn <br/>pip install graphviz <br/>pip install numpy <br/>pip install pandas <br/>pip install matplotlib <br/>pip install tabulate</code></pre>
          </li>
          <li>
            <p>The project can be run by python3 main.py whilst in the project folder. For a list of configurable options (including forrest size, method, and test data) run<p>
            <pre><code>python3 main.py -h </code></pre>
          </p>
        </ol> 

       

        <h2>Reasoning behind project</h2>

        <p>While searching for a project topic, our group decided we wanted to walk away with something applicable to our average Middlebury student, but have some kind of impact beyond this project. We decided upon using a dataset created by former Sociology Professor Peggy Nelson for her research methods class up until 2020.</p>

        <p>The dataset contains 987 responses which although is small compared to other dataset, it is quite large compared to the Middlebury student population of 2580. The dataset itself is a survey of Middlebury students that asks a variety of social questions from relationship status to GPA. </p>
        <p>In addition to the data we got from the past survey, we also collected 60 responses as well from the current year that we could test our trees against as well.

        <p>Our goal was to create a fun algorithm that can predict the relationship status of Middlebury Students based on their response to a series of non-academic, social questions about preferences and beliefs at Middlebury.</p>

        <h2>The Data</h2>
        <p>We gained the survey from a 2011-2020 survey by Professor Peggy Nelson that contained 987 responses. It has 18 questions which are:  <br/>
        <ol>
          <li>Are you currently in a relationship? (yes/no)</li>
          <li>Middlebury provides me with opportunities to find someone with whom to have a relationship (1-5)</li>
          <li>Middlebury provides me with opportunities to find someone with whom to hook up (1-5)</li>
          <li>The kind of person I ideally want a relationship with goes to Middlebury (1-5)</li>
          <li>The kind of person I ideally want to hook up with goes to Middlebury (1-5)</li>
          <li>I am actively looking for someone to have a relationship with at Middlebury (1-5)</li>
          <li>I am actively looking for someone to hook-up with at Middlebury (1-5)</li>
          <li>I am satisfied with the opportunities I have to meet new people at Middlebury (1-5)</li>
          <li>The potential for marriage is important before I date someone (1-5)</li>
          <li>Where do you think you are most likely to find a marriage or long-term partner(s)? (Friends/Family/Graduate School/Job or Career/Middlebury/Other)</li>
          <li>Respondent's self-reported gender (Male/Female/Other)</li>
          <li>Self-reported grade point average (0.0 to 4.0 scale) </li>
          <li>Self-reported social class (Lower/Middle/Upper Middle/Upper)</li>
          <li>Number of siblings</li>
          <li>Are your parents married? (yes/no)</li>
          <li>Self-reported race </li>
          <li>Where do you live this semester? (Dorm/House/Off) </li>
          <li>Class year at Middlebury (First/Sophomore/Junior/Senior) </li>
        </ol>

        <h2>Basic overview of how our code works</h2>

        <p>Our code is split into 3 parts: the main class, the Decision tree class, and the Random Forrest class. <br/>
        The Decision Tree class can build a tree, render a tree, and classify an observation based on a created tree</p>

        <ul>
          <li>
          <p> This is our Decision Tree Class Methods</p>
            <pre><code>class DecisionTree:<br/> <br/>#Constructor for Decision Tree method<br/>def __init__(self, features, classification, method, id)<br/><br/>#Build tree method<br/>def buildTree(self) <br/><br/>#Renders a tree using graphviz and matplotlib<br/>renderTree(self)<br/><br/>#Classifies a new observation<br/>def classifyTree(self, observation)
            </code></pre>
          </li>
          <li>
          <p> This is our Random Forest Class Methods </p>
            <pre><code>class RandomForest: </br><br/>#constructor for the Random Forest Method<br/>def __init__(self, numtrees, trainingfeat, trainingclass, method)<br/><br/>#Builds the Random Forest using the train dataset<br/>def buildForest(self)<br/><br/>#Classifies an observation using the Random Forest that we built previously<br/>def classfiyObservation(self, observation, method)<br/><br/>#Tests the Accuracy of our model on the testing data<br/>def testAccuracy(self, testingfeat, testingclass)<br/><br/>#Finds the optimal tuning parameters<br/>def findOptimal(forest, trainingfeat, trainingclass, splitmethod, testingfeat, testingclass)<br/><br/>printAccuracy(self)
            </code></pre>
          </li>
          <li>
            <p>Our main class is called when program is executed. It parses the arguments passed by the user through command line and trains the model</p>
          </li>
        </ul> 

        <h2>Random Forest Algorithm Explained</h2>
          <p>Random Forests are way to obtain greater accuracy with decision trees. In essence, it works by simulate a ton of decision trees and return the majority classification of the testing data among them. Each decision tree is trained on random data entries with replacement from the training set and with a random set of features. The idea is to have enough trees so that outlying data and features don't have too much of an influence on classification. There is some randomness in random forests so it might not classify data in the same way each time.<br/>We also use two different methods to create our decision trees: Gini and Entropy. </p>
          <p> Gini Index is calculated: 1 - &#931 p^2 </p>
          <p> Entropy is calculated:  - &#931 p * log(p) </p>
          <p> It depends on the data set on which one works better. We didn't find a huge difference between the two.</p>

        <h2>Tree Parameters and Optimization</h2>
          <p> The trees in our Random Forest can take in different parameters. These include Max Depth, Min Sample Split, Min Impurity Decrease. </p>
          <dl>
            <dt>Max Depth</dt>
            <dd>This is the max depth of our tree. Having the largest tree is not always recommended because it can lead to overfitting which essentially means that data gets misclassified because to many splits occur.</dd> <br/>
            <dt>Min Sample Split</dt>
            <dd>This is the minimum number of times that the tree should split. This tackles underfitting of trees. Essentially, if a tree splits too little then it misclassify data because a lot of data is classified together</dd> <br/>
            <dt>Min Impurity Decrease</dt>
            <dd>The gini method uses an impurity calculation to decide which features to split on. The min impurity decrease decides if theres enough of an impurity to split</dd>
          </dl>
          <p>We created a find optimal method in our Random Forest class that attempts to find the best possible tuning parameters. The possible Max Depth options it tests are 2, 5, 8, 11, None; the possible Mine Sample Split options it tests are 2, 5, 10, 20; lastly, the possible min impurity decreases that are tested are: 0 to 0.2 with an increment of .02</p>
          <p>After aprox. 3 hours of running, we found that our optimal accuracy was 90.63% with a max depth of 8, a min sample split of 20, and a min impurity decrease of 0.0. This is much better than our lowest accuracy of 82.3%.</p>

        <h2>Algorithm performance</h2>
          <table>
            <tbody>
          <tr>
            <th>Method</th>
            <th>Number of Trees in Forest</th>
            <th>RunTime</th>
            <th>Accuracy</th>
          </tr>
          <tr>
            <td>Gini</td>
            <td>300</td>
            <td>Alot?</td>
            <td>Alot?</td>
          </tr>
          </tbody>
          </table>
          <h2>Our Project Conclusion </h2>
          <p>Open Picture Below in New Tab to See Whole Tree</p>
          <img src="bigtree.png" alt="tree render">
          <p>The three questions which had the highest importance on a students relationship status were: looking for a relationship, looking for a hookup, and gpa. The first two were expected since a student wouldn’t often be looking for relationship if they were already in one; similarly, a student wouldn’t look for a hook up if they were in a relationship. GPA had unexpectedly high importance. Overall, people in relationships tended to have lower GPA’s, however, the high importance seems to also be because of scale nature GPA’s which allows it to split at a very specific point. Other interesting questions with relatively high importance were: siblings, being white, being a woman, and being satisfied with the amount of new people one meets at meet. In future studies, one could dive into these questions with greater depth and try to understand why they held such importance. <br/> <br/>
          On the algorithm side, our optimal accuracy reached was 90.63% and had the parameters of a max depth of 8, a minimum sample split of 20, and a minimum impurity decrease of 0.0. This accuracy was significantly better than the lowest accuracy of 82.3%.
          </p>
          <p class="view"><em>For the full list of question importance, additional info, and detailed analysis see our final paper on <a href="https://docs.google.com/document/d/1O1phRgtGrBLben_A7D-Lv9dGYZ7l5m8MHdMpZKAGLLM/edit?usp=sharing">Google Docs. </a></em></p>

        <h2>Example Tree Renderings </h2>
        <img src="treerender5.png" alt="tree render">
        <img src="treerender7.png" alt="tree render">

        <h2>Bugs and difficulties</h2>

         <dl>
          <dt>Getting used to python</dt>
          <dd>One of our initial difficulties was getting used to python and the python libraries that we had to use. These included python with sci-learn, numpy, pandas, graphviz, and matplotlib.</dd>
          <dt>Preprocessing Data</dt>
          <dd>Our data was not immediately usable so we had to figure out a way to make it into the right format</dd>
          <dt>Figuring out how sklearn and pandas</dt>
          <dd>We had some trouble halfway through the project with the way that sklearn and pandas handled data. Specifically, pandas worked with dataframes which are slightly different than other data structures that we are used to </dd>
          <dt>Long run time</dt>
          <dd>Our project took a significant time to run which meant that if code was wrong or parameters were set incorrectly, it would take us a while to find out. </dd>
        </dl>

      </section>
      <footer>
        <p>This project is maintained by <a href="https://github.com/Hokalaka2/AIFinalWebsite">Otis Milliken and Quin McGaugh</a></p>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
  </body>
</html>
